{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d43766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üß† 03_sentiment_topics_analysis.ipynb\n",
    "# Multilingual Sentiment + Topic extraction for App Reviews\n",
    "# ============================================================\n",
    "\n",
    "# ‚úÖ Requirements (run once)\n",
    "%pip install pandas numpy transformers spacy keybert sentence-transformers langdetect tqdm\n",
    "\n",
    "# For language support (download models)\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!python -m spacy download it_core_news_sm\n",
    "#!python -m spacy download fr_core_news_sm\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from langdetect import detect\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ============================================================\n",
    "# üìÅ Resolve project paths\n",
    "# ============================================================\n",
    "\n",
    "def find_project_root(marker=\"data\"):\n",
    "    root = Path.cwd()\n",
    "    while not (root / marker).exists() and root != root.parent:\n",
    "        root = root.parent\n",
    "    if not (root / marker).exists():\n",
    "        raise FileNotFoundError(\"Could not locate project root containing data/.\")\n",
    "    return root\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "CONFIG_DIR = PROJECT_ROOT / \"config\"\n",
    "OUTPUT_DIR = DATA_DIR / \"output\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783c406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üì• 1. LOAD DATA\n",
    "# ============================================================\n",
    "\n",
    "INPUT_PATH = DATA_DIR / \"ml\" / \"processed_reviews.csv\"\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "# Keep only useful columns\n",
    "df = df[['id','app_name','country','rating','cleaned_content','review_date']]\n",
    "\n",
    "print(f\"Loaded {len(df)} reviews\")\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ea4708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üåç 2. DETECT LANGUAGE (fallback if not already in dataset)\n",
    "# ============================================================\n",
    "\n",
    "if 'language' not in df.columns:\n",
    "    tqdm.pandas()\n",
    "    df['language'] = df['cleaned_content'].progress_apply(lambda x: detect(x) if isinstance(x, str) else 'unknown')\n",
    "\n",
    "print(df['language'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143e4ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ‚úÇÔ∏è 3. SPLIT LONG REVIEWS INTO SENTENCES\n",
    "# ============================================================\n",
    "\n",
    "# Load multilingual spaCy models on demand\n",
    "CONFIG_PATH = CONFIG_DIR / \"apps.json\"\n",
    "if CONFIG_PATH.exists():\n",
    "    with open(CONFIG_PATH) as f:\n",
    "        cfg = json.load(f)\n",
    "        COUNTRIES = cfg.get(\"countries\", [])\n",
    "else:\n",
    "    # Option B ‚Äî fallback from dataset\n",
    "    COUNTRIES = df[\"country\"].unique().tolist()\n",
    "\n",
    "# Country ‚Üí language mapping\n",
    "COUNTRY_LANG_MAP = {\n",
    "    \"fr\": \"fr\",\n",
    "    \"us\": \"en\",\n",
    "    \"gb\": \"en\",\n",
    "    \"ca\": \"en\",  # could also be 'fr'\n",
    "    \"de\": \"de\",\n",
    "    \"se\": \"sv\",  # no direct model; fallback to English\n",
    "    \"it\": \"it\",\n",
    "    \"es\": \"es\",\n",
    "}\n",
    "\n",
    "# Prepare language list from dataset\n",
    "languages_to_load = sorted(set(COUNTRY_LANG_MAP.get(c, \"en\") for c in COUNTRIES))\n",
    "print(\"Detected languages:\", languages_to_load)\n",
    "\n",
    "# Load only what we need\n",
    "models = {}\n",
    "for lang in languages_to_load:\n",
    "    try:\n",
    "        if lang == \"sv\":  # fallback for Swedish\n",
    "            models[lang] = spacy.load(\"en_core_web_sm\")\n",
    "        else:\n",
    "            models[lang] = spacy.load(f\"{lang}_core_news_sm\")\n",
    "    except OSError:\n",
    "        print(f\"‚ö†Ô∏è Missing spaCy model for {lang}, using English fallback.\")\n",
    "        models[lang] = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(models)} language models:\", list(models.keys()))\n",
    "\n",
    "def split_sentences(text, lang_code):\n",
    "    lang_code = COUNTRY_LANG_MAP.get(lang_code, lang_code)\n",
    "    nlp = models.get(lang_code, models.get(\"en\"))\n",
    "    doc = nlp(str(text))\n",
    "    return [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "df['sentences'] = df.progress_apply(lambda r: split_sentences(str(r.cleaned_content), r.language), axis=1)\n",
    "df['n_sentences'] = df['sentences'].apply(len)\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ec6738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üí¨ 4. SENTIMENT ANALYSIS (multilingual tolerant)\n",
    "# ============================================================\n",
    "\n",
    "sentiment_model = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained(sentiment_model)\n",
    "MAX_TOKENS = sentiment_tokenizer.model_max_length  # keep inputs within model limit\n",
    "\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=sentiment_model,\n",
    "    tokenizer=sentiment_tokenizer,\n",
    ")\n",
    "\n",
    "def analyze_sentiments(sent_list, batch_size=32):\n",
    "    sentences = [s for s in sent_list if isinstance(s, str) and s.strip()]\n",
    "    if not sentences:\n",
    "        return []\n",
    "\n",
    "    labels = []\n",
    "    for start in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[start:start + batch_size]\n",
    "        outputs = sentiment_analyzer(\n",
    "            batch,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=MAX_TOKENS,\n",
    "        )\n",
    "        labels.extend(pred[\"label\"] for pred in outputs)\n",
    "    return labels\n",
    "\n",
    "tqdm.pandas()\n",
    "df[\"sentence_sentiments\"] = df[\"sentences\"].progress_apply(analyze_sentiments)\n",
    "\n",
    "\n",
    "def aggregate_sentiments(labels):\n",
    "    pos = sum(\"POS\" in l.upper() for l in labels)\n",
    "    neg = sum(\"NEG\" in l.upper() for l in labels)\n",
    "    total = pos + neg\n",
    "    \n",
    "    # No polarity at all\n",
    "    if total == 0:\n",
    "        return \"neutral\"\n",
    "    \n",
    "    ratio = pos / total\n",
    "\n",
    "    # Define smooth thresholds\n",
    "    if 0.4 <= ratio <= 0.6:\n",
    "        return \"mixed\"\n",
    "    elif ratio > 0.6:\n",
    "        return \"positive\"\n",
    "    else:\n",
    "        return \"negative\"\n",
    "\n",
    "\n",
    "df['sentiment_mode'] = df['sentence_sentiments'].apply(aggregate_sentiments)\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1ca0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üß© 5. TOPIC EXTRACTION (KeyBERT using SentenceTransformer)\n",
    "# ============================================================\n",
    "\n",
    "kw_model = KeyBERT(model=SentenceTransformer(\"all-MiniLM-L6-v2\"))\n",
    "\n",
    "def extract_topics_per_sentence(sent_list, lang):\n",
    "    topics = []\n",
    "    for s in sent_list:\n",
    "        try:\n",
    "            kw = kw_model.extract_keywords(s, keyphrase_ngram_range=(1,2), stop_words='english', top_n=2)\n",
    "            topics.append(\"; \".join([k for k, _ in kw]))\n",
    "        except:\n",
    "            topics.append(\"\")\n",
    "    return topics\n",
    "\n",
    "tqdm.pandas()\n",
    "df['sentence_topics'] = df.progress_apply(lambda r: extract_topics_per_sentence(r.sentences, r.language), axis=1)\n",
    "\n",
    "# Aggregate top topics per review\n",
    "def merge_topics(topics_lists):\n",
    "    tokens = []\n",
    "    for t in topics_lists:\n",
    "        if isinstance(t, list):\n",
    "            tokens.extend(t)\n",
    "    flat = \"; \".join(tokens)\n",
    "    unique = list(dict.fromkeys([x.strip() for x in flat.split(';') if x.strip()]))\n",
    "    return \"; \".join(unique[:5])\n",
    "\n",
    "df['topics'] = df['sentence_topics'].apply(merge_topics)\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa923b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üß† 6. STRUCTURE PER-REVIEW SENTIMENT/TOPIC TABLE\n",
    "# ============================================================\n",
    "\n",
    "def sentence_details(sentences, sentiments, topics):\n",
    "    return [\n",
    "        {\"sentence\": s, \"sentiment\": sen, \"topics\": t}\n",
    "        for s, sen, t in zip(sentences, sentiments, topics)\n",
    "    ]\n",
    "\n",
    "df['details'] = df.apply(lambda r: sentence_details(r.sentences, r.sentence_sentiments, r.sentence_topics), axis=1)\n",
    "\n",
    "structured_df = df[['id','app_name','country','language','rating','cleaned_content',\n",
    "                    'sentiment_mode','topics','details','review_date']]\n",
    "\n",
    "reviews_export_path = OUTPUT_DIR / \"reviews_sentiment_topics.csv\"\n",
    "structured_df.to_csv(reviews_export_path, index=False)\n",
    "print(f\"‚úÖ Exported {len(structured_df)} rows to {reviews_export_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62aae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üó£Ô∏è 7. GENERATE NOTEBOOKLM SENTENCE SUMMARIES\n",
    "# ============================================================\n",
    "\n",
    "def notebook_sentence(row):\n",
    "    lang_info = f\"In {row['country'].upper()}, a {row['app_name'].capitalize()} user wrote in {row['language']}.\"\n",
    "    tone = f\"The overall sentiment is {row['sentiment_mode']}.\"\n",
    "    topic_part = f\"It mainly discusses: {row['topics']}.\"\n",
    "    quotes = \" \".join([f\"'{d['sentence']}' [{d['sentiment']}]\" for d in row['details']])\n",
    "    return f\"{lang_info} {tone} {topic_part} Example sentences: {quotes}\"\n",
    "\n",
    "df['notebook_sentence'] = df.apply(notebook_sentence, axis=1)\n",
    "\n",
    "notebook_df = df[['id','app_name','country','sentiment_mode','topics','notebook_sentence']]\n",
    "notebook_export_path = OUTPUT_DIR / \"notebooklm_reviews.csv\"\n",
    "notebook_df.to_csv(notebook_export_path, index=False)\n",
    "print(f\"‚úÖ Exported NotebookLM-ready summaries to {notebook_export_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947772f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üìä 8. OPTIONAL ‚Äî TOPIC SUMMARY TABLE (for NotebookLM clustering)\n",
    "# ============================================================\n",
    "\n",
    "summary = (\n",
    "    structured_df\n",
    "    .explode('topics')\n",
    "    .groupby(['app_name','country','topics','sentiment_mode'])\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    ")\n",
    "\n",
    "topic_summary_path = OUTPUT_DIR / \"topic_summary.csv\"\n",
    "summary.to_csv(topic_summary_path, index=False)\n",
    "print(f\"‚úÖ Exported aggregated topic summary to {topic_summary_path}\")\n",
    "summary.head(5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
