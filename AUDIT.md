# ğŸ“Š Pipeline Audit Report (Codex)

## âœ… Working as expected
- `scripts/01_scrape.py` creates the `data/raw` directory before saving outputs and persists per-run summaries, so downstream steps have a predictable input location.ã€F:scripts/01_scrape.pyâ€ L10-L107ã€‘
- `scripts/02_process_reviews.py` provisions the `data/processed`, `data/logs`, and `data/metadata` folders, performs text normalization, language detection, and adds enrichment metrics to each cleaned dataset.ã€F:scripts/02_process_reviews.pyâ€ L16-L158ã€‘
- The Supabase upload step reads from processed CSVs, uses service-role credentials, and upserts into `clean_reviews` with a conflict target that matches the unique index defined in the schema.ã€F:scripts/03_upload_to_supabase.pyâ€ L6-L25ã€‘ã€F:sql/001_create_tables.sqlâ€ L2-L19ã€‘
- GitHub Actions installs dependencies, runs the scrape â†’ process â†’ upload scripts sequentially, and wires Supabase secrets into the uploader step, mirroring the intended local workflow.ã€F:.github/workflows/pipeline.ymlâ€ L8-L36ã€‘

## âš ï¸ Issues / Inconsistencies
- The scraper hardcodes `config/apps.json`, but the repository does not ship a `config/` directory, so the very first pipeline step will raise `FileNotFoundError` when run from a fresh clone.ã€F:scripts/01_scrape.pyâ€ L10-L83ã€‘
- Processed filenames are emitted as `{app}_{country}_clean_{timestamp}.csv`, yet the uploader only scans for `*_clean.csv`, so no processed file will be discovered or upserted.ã€F:scripts/02_process_reviews.pyâ€ L155-L158ã€‘ã€F:scripts/03_upload_to_supabase.pyâ€ L22-L25ã€‘
- The processor writes columns named `lang`, `year_month`, and `review_length`, but these do not exist on `clean_reviews`; sending them in an upsert will produce a PostgREST error. Likewise, the processor never fills the `language` or `cleaned_content` fields the table defines (it overwrites `content` in place).ã€F:scripts/02_process_reviews.pyâ€ L120-L154ã€‘ã€F:sql/001_create_tables.sqlâ€ L2-L13ã€‘
- Because language is stored under `lang` instead of `language`, the Supabase table will keep that column `NULL`, breaking downstream views or filters that expect populated values.ã€F:scripts/02_process_reviews.pyâ€ L138-L141ã€‘ã€F:sql/001_create_tables.sqlâ€ L2-L13ã€‘
- The SQL view aggregates on `review_date`, but the processor only coerces timestamps when a column containing "date" exists; if raw files omit or rename this field, the view will return NULL days without safeguards or logging.ã€F:scripts/02_process_reviews.pyâ€ L144-L149ã€‘ã€F:sql/002_views_and_indexes.sqlâ€ L1-L10ã€‘

## ğŸ§© Suggestions for improvement
- Commit a sample `config/apps.json` template (or guard `load_config()` with clearer error messaging) so first runs succeed without manual file creation.ã€F:scripts/01_scrape.pyâ€ L10-L83ã€‘
- Align processed filenames and uploader globbingâ€”either change the processor to emit `*_clean.csv` or expand the uploader pattern (e.g., `*_clean_*.csv`).ã€F:scripts/02_process_reviews.pyâ€ L155-L158ã€‘ã€F:scripts/03_upload_to_supabase.pyâ€ L22-L25ã€‘
- Normalize column names before upload: rename `lang` â†’ `language`, persist the pre-clean text as `content`, store the cleaned string in `cleaned_content`, and drop helper columns (`year_month`, `review_length`) or create matching Supabase fields/views.ã€F:scripts/02_process_reviews.pyâ€ L120-L154ã€‘ã€F:sql/001_create_tables.sqlâ€ L2-L13ã€‘
- Add schema-aware validation in the uploader (e.g., check for required keys, cast dates to ISO strings) and wrap the upsert call with error handling to surface PostgREST responses early.ã€F:scripts/03_upload_to_supabase.pyâ€ L11-L20ã€‘
- Extend the view to guard against NULL `review_date` rows or have the processor default missing review dates to the scrape day to keep aggregates stable.ã€F:scripts/02_process_reviews.pyâ€ L144-L149ã€‘ã€F:sql/002_views_and_indexes.sqlâ€ L1-L10ã€‘

## ğŸ’¡ Optional enhancements
- Batch large uploads (e.g., chunk records) and add simple retry logic with exponential backoff for transient Supabase errors.ã€F:scripts/03_upload_to_supabase.pyâ€ L11-L20ã€‘
- Emit structured logs (JSON or CSV) from each stage and include them in the GitHub Action artifact to aid post-run diagnostics.ã€F:scripts/02_process_reviews.pyâ€ L169-L175ã€‘ã€F:.github/workflows/pipeline.ymlâ€ L32-L36ã€‘
- Store run metadata in Supabase (e.g., `pipeline_runs` table) to track scrape volumes and processing stats alongside review data.ã€F:scripts/01_scrape.pyâ€ L98-L107ã€‘ã€F:scripts/02_process_reviews.pyâ€ L169-L175ã€‘

---

*Generated automatically by Codex on 2024-07-08*
