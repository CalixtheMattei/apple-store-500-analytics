# ğŸ“Š Pipeline Audit Report (Codex)

## âœ… Working as expected
- `config/apps.json` structure (apps, countries, source, scrape delay) matches how `scripts/01_scrape.py` loads configuration and iterates scraping targets, ensuring consistent runtime parameters.ã€F:config/apps.jsonâ€ L1-L13ã€‘ã€F:scripts/01_scrape.pyâ€ L14-L107ã€‘
- The scraper captures `source_review_id`, rating metadata, review text, and source identifiers that align with columns defined in the Supabase `clean_reviews` schema, and persists them under `data/raw/` for downstream steps.ã€F:scripts/01_scrape.pyâ€ L57-L95ã€‘ã€F:sql/001_create_tables.sqlâ€ L4-L15ã€‘
- The Supabase DDL creates a unique index on `(source, app_name, country, source_review_id)`, which matches the `on_conflict` clause used during upsert in `03_upload_to_supabase.py`, preventing duplicate rows on re-scrapes.ã€F:sql/001_create_tables.sqlâ€ L17-L19ã€‘ã€F:scripts/03_upload_to_supabase.pyâ€ L16-L25ã€‘
- The GitHub Actions workflow installs dependencies and runs the three pipeline steps (scrape â†’ process â†’ upload) in the intended order while injecting Supabase secrets for the upload stage.ã€F:.github/workflows/pipeline.ymlâ€ L17-L36ã€‘

## âš ï¸ Issues / Inconsistencies
- Processed CSV filenames include a timestamp suffix (e.g., `*_clean_2025-11-08_20-06-53.csv`), but the uploader only searches for files matching `*_clean.csv`, so no processed data will ever be uploaded.ã€F:scripts/02_process_reviews.pyâ€ L151-L158ã€‘ã€F:scripts/03_upload_to_supabase.pyâ€ L22-L25ã€‘
- The processed dataset introduces a `lang` column and retains helper fields such as `year_month` and `review_length`, none of which exist in the `clean_reviews` table; attempting to upsert these records into Supabase will raise column mismatch errors.ã€F:scripts/02_process_reviews.pyâ€ L138-L154ã€‘ã€F:sql/001_create_tables.sqlâ€ L4-L13ã€‘
- The cleaning step overwrites the `content` column with normalized text but never populates the `cleaned_content` or `language` fields expected by the Supabase schema, causing the upload to omit required analytics columns and fail schema validation.ã€F:scripts/02_process_reviews.pyâ€ L120-L153ã€‘ã€F:sql/001_create_tables.sqlâ€ L10-L12ã€‘
- The uploader relies on raw environment variable access at import time; if the GitHub Action secrets are misconfigured or missing, the module import will crash before `main()` executes, preventing graceful error handling or logging.ã€F:scripts/03_upload_to_supabase.pyâ€ L6-L10ã€‘
- Requirements list includes `dotenv`, but no script loads `.env` filesâ€”locally running the uploader without exporting variables will fail. Either document the expectation or add `python-dotenv` integration for parity with CI secrets.ã€F:requirements.txtâ€ L1-L9ã€‘ã€F:scripts/03_upload_to_supabase.pyâ€ L6-L10ã€‘

## ğŸ§© Suggestions for improvement
- Align processed CSV naming with the uploader by either adjusting the glob to `*_clean_*.csv` or removing the timestamp suffix so uploads run automatically in CI.ã€F:scripts/02_process_reviews.pyâ€ L151-L158ã€‘ã€F:scripts/03_upload_to_supabase.pyâ€ L22-L25ã€‘
- Map processed columns to the Supabase schema before upsert: preserve raw text in `content`, add a separate `cleaned_content`, and rename `lang` â†’ `language` while dropping auxiliary fields (`year_month`, `review_length`) prior to upload.ã€F:scripts/02_process_reviews.pyâ€ L120-L154ã€‘ã€F:sql/001_create_tables.sqlâ€ L10-L12ã€‘
- Add defensive handling around Supabase client creation (e.g., using `os.getenv` with validation inside `main()` and raising a clear message) to improve resilience when secrets are missing or rotated.ã€F:scripts/03_upload_to_supabase.pyâ€ L6-L27ã€‘
- Document or implement local secret loading by using `python-dotenv` (instead of the unused `dotenv` package) so contributors can mirror CI behavior during manual runs.ã€F:requirements.txtâ€ L6-L9ã€‘ã€F:scripts/03_upload_to_supabase.pyâ€ L6-L10ã€‘

## ğŸ’¡ Optional enhancements
- Extend the uploader to chunk large upserts and log Supabase responses for observability, making retries easier to diagnose during CI runs.ã€F:scripts/03_upload_to_supabase.pyâ€ L11-L20ã€‘
- Capture both raw and cleaned text in the processed output (e.g., `raw_content`, `cleaned_content`) for richer downstream analytics and to support schema evolution without re-scraping.ã€F:scripts/02_process_reviews.pyâ€ L120-L154ã€‘
- Publish processed summaries (`run_clean_summary_*.json`) as GitHub Action artifacts or Supabase tables for monitoring ingestion health over time.ã€F:scripts/02_process_reviews.pyâ€ L169-L176ã€‘ã€F:.github/workflows/pipeline.ymlâ€ L32-L36ã€‘

---

*Generated automatically by Codex on 2025-11-08*
